"""
step3_global_constraints.py

Step 3: å…¨å±€çº¦æŸæŠ½å– (Global Constraint Extraction)

- æˆ‘ä»¬æŠŠå…¨å±€çº¦æŸåˆ†æˆä¸¤ç±»ï¼š
  A. ç¡¬æ€§å¯ç¨‹åºæ ¡éªŒçš„å…¨å±€çº¦æŸï¼ˆhard global constraintsï¼‰
     ä¾‹å¦‚ï¼šæœ€å°‘è¯æ•°ã€å¿…é¡»åŒ…å«ç»“æ„æ®µè½ã€ç¦æ­¢ç¬¬ä¸€äººç§°ã€å¿…é¡»ä¸ºè‹±æ–‡ã€‚
     è¿™äº›å¯ä»¥ç›´æ¥ç”±æˆ‘ä»¬æœ¬åœ°ä»£ç ç»™å‡ºå¹¶é™„ä¸Š verifier_specï¼Œ
     ä¸ä¾èµ– LLMï¼Œå› æ­¤æ€»æ˜¯å¯ç”¨ï¼Œä¿è¯ä¸‹æ¸¸è¯„æµ‹æœ‰ç¨³å®šåŸºçº¿ã€‚

  B. è½¯æ€§ / è¯­æ°” / å®‰å…¨ / è´¨é‡ç±»å…¨å±€çº¦æŸï¼ˆsoft / semantic global constraintsï¼‰
     ä¾‹å¦‚ï¼šä¸­ç«‹åˆ†æè¯­æ°”ã€ä¸å¾—ç…½åŠ¨æ€§æ”»å‡»ã€è¾“å‡ºå¿…é¡»ä¿æŒä¸“ä¸šè€Œéæƒ…ç»ªåŒ–ã€‚
     è¿™äº›éœ€è¦è¯­ç”¨åˆ¤æ–­ï¼Œç»§ç»­è°ƒç”¨ deepseek ç”Ÿæˆæˆ–ç¡®è®¤ï¼Œ
     å¹¶ä¸ºå®ƒä»¬é™„ä¸Š LLM-based çš„ verifierï¼ˆå¦‚ tone_neutral_llm_judge, non_extremeness_judgeï¼‰ã€‚


è¾“å‡ºï¼šList[ConstraintNode]
- æ¯ä¸ª ConstraintNode:
    cid: å…¨å±€å”¯ä¸€IDï¼ˆG1, G2, ...ï¼‰
    desc: äººç±»å¯è¯»æè¿°
    scope: "global"
    verifier_spec: {"check": <fn-name>, "args": {...}}
    derived_from: "step3"

ä¾èµ–ï¼š
- deepseek-chat (ç”¨äºè½¯æ€§çº¦æŸ)
- ConstraintNode schema
- ç¡¬æ€§è§„åˆ™æ¥è‡ªæˆ‘ä»¬è‡ªå·±çš„å¯å‘å¼ï¼š
  - å­—æ•°ä¸‹é™ (min_word_count)
  - è¯­è¨€åˆ¤æ–­ (require_language)
  - ç»“æ„æ®µè½ (has_sections) [ä»…å½“å›ç­”æ˜æ˜¾åˆ†å—æ—¶]
  - ç¦æ­¢ç¬¬ä¸€äººç§° (forbid_first_person) [å¯é€‰]
"""

import json
import requests
from typing import List, Dict, Any

from .graph_schema import ConstraintNode
from .utils.parsing import extract_constraints
from .utils.text_clean import make_snippet, summarize_blocks_outline, clip

_DEEPSEEK_API_KEY_DEFAULT = "sk-4bb3e24d26674a30b2cc7e2ff1bfc763"
_DEEPSEEK_ENDPOINT = "https://api.deepseek.com/v1/chat/completions"
_DEEPSEEK_MODEL = "deepseek-chat"


# -------------------------------------------------
# å·¥å…·ï¼šä»å½“å‰å›ç­”ä¸­æ¨æµ‹ç¡¬æ€§å…¨å±€çº¦æŸåŸºçº¿
# -------------------------------------------------

def _estimate_word_count(text: str) -> int:
    import re
    tokens = re.findall(r"\w+", text)
    return len(tokens)


def _guess_language(text: str) -> str:
    """
    ç²—æš´åˆ¤æ–­æ–‡æœ¬ä¸»è¦è¯­è¨€ï¼š
    - å¦‚æœåŒ…å«è¾ƒå¤šä¸­æ–‡æ±‰å­— => 'zh'
    - å¦åˆ™é»˜è®¤ 'en'
    æˆ‘ä»¬ä¸åšå¤æ‚æ£€æµ‹ï¼Œè¿™åªæ˜¯ä¸ºäº†æ„é€  require_languageã€‚
    """
    import re
    zh_chars = re.findall(r"[\u4e00-\u9fff]", text)
    if len(zh_chars) >= 10:
        return "zh"
    return "en"


def _has_intro_body_conclusion(segmentation: Dict[str, Any]) -> bool:
    """
    æ ¹æ® Step2 çš„ segmentation ç»“æœï¼Œçœ‹çœ‹æ˜¯å¦èƒ½è§‚å¯Ÿåˆ°å…¸å‹ç»“æ„ï¼š
    - å­˜åœ¨å¼€ç¯‡ç±»å— (Opening / Intro / Background / Context)
    - å­˜åœ¨ä¸»ä½“åˆ†æç±»å— (Main Analysis / Discussion / Evaluation / Argument)
    - å­˜åœ¨æ€»ç»“/å±•æœ›ç±»å— (Conclusion / Summary / Outlook / Recommendation)

    å¦‚æœè¿™äº›intentåŸºæœ¬å­˜åœ¨ï¼Œå°±å¯ä»¥ç”Ÿæˆä¸€ä¸ª has_sections çº¦æŸã€‚
    å¦åˆ™åˆ«å¼ºè¡Œè¦æ±‚ã€‚
    """
    intents = [blk.get("intent", "").lower() for blk in segmentation.get("blocks", [])]

    def any_contains(keys):
        return any(any(k in intent for k in keys) for intent in intents)

    has_opening = any_contains(["opening", "intro", "context", "background"])
    has_body = any_contains(["analysis", "discussion", "main", "argument", "evaluation"])
    has_conclusion = any_contains(["conclusion", "summary", "outlook", "recommendation"])

    return has_opening and has_body and has_conclusion


def _build_hard_global_constraints(response_text: str,
                                   segmentation: Dict[str, Any]) -> List[ConstraintNode]:
    """
    åŸºäºå¯è§‚æµ‹ä¿¡å·ï¼Œæ„é€ ç¨³å®šçš„ç¡¬æ€§å…¨å±€çº¦æŸèŠ‚ç‚¹ã€‚
    æˆ‘ä»¬ä¸ä¼šå¹»æƒ³ä¸å­˜åœ¨çš„è¦æ±‚ï¼Œåªæ ¹æ®æ–‡æœ¬æœ¬èº«çš„å®¢è§‚å±æ€§ï¼š
    - å­—æ•°ä¸‹é™ï¼šè®¾ä¸º floor(word_count * 0.8) å‘ä¸‹å–æ•´ï¼Œä½†è‡³å°‘ 100 è¯ã€‚
      ï¼ˆæ€è·¯ï¼šæˆ‘ä»¬å¸Œæœ›åç»­å›ç­”åˆ«æ¯”ç¤ºä¾‹çŸ­å¤ªå¤šï¼Œå¦åˆ™ä¸åˆæ ¼ï¼‰
    - è¯­è¨€ï¼šæ ¹æ®æ–‡æœ¬ä¸»è¯­è¨€ç”Ÿæˆ require_language(lang=...)
    - ç»“æ„æ®µè½ï¼šå¦‚æœ segmentation çœ‹èµ·æ¥æœ‰å¼€å¤´/ä¸»ä½“/ç»“è®ºï¼Œå°±è¦æ±‚ has_sections
    è¿™äº›éƒ½ä¼šè¢«æ ‡è®°ä¸º scope="global"ã€‚
    """
    nodes: List[ConstraintNode] = []
    cid_counter = 1

    # 1. å­—æ•°ä¸‹é™çº¦æŸ
    wc = _estimate_word_count(response_text)
    if wc > 0:
        target_min = max(100, int(wc * 0.8))
        nodes.append(
            ConstraintNode(
                cid=f"G{cid_counter}",
                desc=f"The answer must be at least {target_min} words long (approximately comparable length to the provided reference).",
                scope="global",
                verifier_spec={
                    "check": "min_word_count",
                    "args": {"min_words": target_min},
                },
                trace_to=None,
                derived_from="step3",
            )
        )
        cid_counter += 1

    # 2. ä¸»è¯­è¨€çº¦æŸ
    lang = _guess_language(response_text)
    nodes.append(
        ConstraintNode(
            cid=f"G{cid_counter}",
            desc=("The answer must be written primarily in Chinese." if lang == "zh"
                  else "The answer must be written primarily in English."),
            scope="global",
            verifier_spec={
                "check": "require_language",
                "args": {"lang": lang},
            },
            trace_to=None,
            derived_from="step3",
        )
    )
    cid_counter += 1

    # 3. ç»“æ„æ€§çº¦æŸï¼ˆä»…å½“å›ç­”çœŸçš„æœ‰æ˜æ˜¾ç»“æ„ï¼‰
    if _has_intro_body_conclusion(segmentation):
        nodes.append(
            ConstraintNode(
                cid=f"G{cid_counter}",
                desc="The answer must include an Opening/Intro section, a Main Analysis/Body section, and a Conclusion/Outlook section in logical progression.",
                scope="global",
                verifier_spec={
                    "check": "has_sections",
                    "args": {"sections": ["Opening", "Body", "Conclusion"]},
                },
                trace_to=None,
                derived_from="step3",
            )
        )
        cid_counter += 1

    # 4. ç¦ç”¨ç¬¬ä¸€äººç§°ï¼ˆåªåœ¨å›ç­”ä¸»è¦æ˜¯ç¬¬ä¸‰äººç§°åˆ†æé£æ ¼æ—¶æ·»åŠ ï¼‰
    # å¯å‘å¼ï¼šå¦‚æœæ–‡æœ¬é‡Œå‡ ä¹æ²¡æœ‰ "I " / "we ", æˆ‘ä»¬å‡å®šå®ƒæ˜¯å®¢è§‚ç¬¬ä¸‰äººç§°åˆ†æï¼Œ
    # é‚£ä¹ˆæˆ‘ä»¬å°±å¯ä»¥æŠŠ forbid_first_person è®¾ä¸ºä¸€ä¸ªçº¦æŸã€‚
    lower_txt = response_text.lower()
    first_person_hits = any(token in lower_txt for token in [" i ", " we ", " my ", " our "])  # ç²—æš´å¯å‘å¼
    if not first_person_hits:
        nodes.append(
            ConstraintNode(
                cid=f"G{cid_counter}",
                desc="The answer should maintain an objective, third-person analytic voice without using first-person pronouns.",
                scope="global",
                verifier_spec={
                    "check": "forbid_first_person",
                    "args": {},
                },
                trace_to=None,
                derived_from="step3",
            )
        )
        cid_counter += 1

    return nodes


# -------------------------------------------------
# LLM: ç”Ÿæˆè½¯æ€§/è¯­æ°”/å®‰å…¨ç±»å…¨å±€çº¦æŸ
# -------------------------------------------------


def _call_deepseek_soft_constraints(response_text: str,
                                    segmentation: Dict[str, Any]) -> str:
    """
    è°ƒç”¨ deepseek è®©å®ƒç»™å‡ºâ€œè¯­æ°” / å®‰å…¨ / é£æ ¼â€ç±»å…¨å±€çº¦æŸã€‚

    éå¸¸é‡è¦ï¼š
    - æˆ‘ä»¬ç°åœ¨è¦æ±‚å®ƒåªèƒ½æ ¹æ®å›ç­”æœ¬èº«å·²ç»å‘ˆç°å‡ºæ¥çš„é£æ ¼/è¯­æ°”/å§¿æ€æ¥æç‚¼çº¦æŸï¼Œ
      ä¸èƒ½è„‘è¡¥â€œç†æƒ³ä¸Šåº”è¯¥æ˜¯ä»€ä¹ˆæ ·â€ã€‚
    - æˆ‘ä»¬æä¾›ç»™å®ƒçš„ TEXT SNIPPET æ˜¯åŸæ–‡æœ¬èº«ï¼ˆä»…åšç©ºç™½è§„æ•´ï¼‰ï¼Œ
      ä¸æ‘˜è¦ã€ä¸æ”¹å†™ã€ä¸è‡ªåŠ¨æˆªæ–­ï¼›åªæœ‰åœ¨æç«¯è¿‡é•¿æ—¶æ‰ clip() ä»¥é˜² token çˆ†ç‚¸ã€‚
    - outline åªæ˜¯ç»“æ„ä½ç½®å‚è€ƒï¼Œä¸èƒ½ç”¨æ¥å‘æ˜æ²¡å‡ºç°çš„è¦æ±‚ã€‚

    æœŸæœ› deepseek è¾“å‡ºï¼šJSON listï¼Œæ¯é¡¹ç±»ä¼¼ï¼š
        {
          "desc": "The answer must maintain a neutral, analytical tone.",
          "verifier": {"check": "tone_neutral_llm_judge", "args": {}}
        }
    """

    # å¤„ç†åŸæ–‡ï¼šä¿æŒè¯­ä¹‰ï¼Œå»é™¤å¤šä½™ç©ºç™½ï¼Œä¸é»˜è®¤æˆªæ–­
    answer_clean = make_snippet(response_text)
    if len(answer_clean) > 20000:
        # æç«¯é•¿æ–‡æœ¬æ‰è§¦å‘æˆªæ–­ï¼›è¿™æ˜¯æ˜¾å¼çš„ã€å¯å®¡è®¡çš„å†…å®¹ä¸¢å¤±ç‚¹
        answer_clean = clip(answer_clean, 20000)

    outline_str = summarize_blocks_outline(segmentation)
    
    system_prompt = """You are an instruction analyst.
Your job is to infer ONLY global style/tone/safety requirements that the FULL ANSWER is ALREADY FOLLOWING.
You MUST base every requirement on observable evidence in the provided TEXT SNIPPET.
Do NOT invent idealized rules that are not clearly demonstrated in that text.
The OUTLINE is just structural context (which block does what), NOT evidence.
If you cannot justify a requirement from the snippet, you must NOT output it.

Soft global constraints are about tone, safety, stance, professional voice, neutrality, actionability, or analyst persona consistency across the entire answer.
Do NOT restate local factual obligations (e.g. "must list three risks") that only apply to one block; those belong to local block constraints, not global style.  ğŸ”

Every constraint must be grounded in observable evidence in the TEXT SNIPPET.
Do NOT invent requirements that do not clearly appear in the text.

You must return ONLY valid JSON: a list of objects.
Each object MUST have: {desc, verifier:{check,args}}.

About verifier.check:
- If one of these fits, use it:
  tone_neutral_llm_judge
  tone_negative_llm_judge
  non_extremeness_judge
  role_consistency_judge
  actionability_judge
- Otherwise, you MUST create a new descriptive snake_case name
  that reflects the requirement, e.g. "must_include_case_studies", "balanced_argumentation", "risk_mitigation_guidance".
  This is allowed.
Any new verifier.check you create MUST still describe a requirement that is clearly exhibited by the TEXT SNIPPET. ğŸ”
You are NOT allowed to invent a requirement that the snippet does not follow, just to create a new check name. ğŸ”

Rules for new verifier names:
- snake_case only [a-z0-9_]
- It must reflect the obligation in desc.
- args must be a JSON object (possibly empty) describing any parameters needed to check this rule, e.g. {"min_items": 3}.

If nothing applies, return an empty JSON list [].

Rules:
- desc must be English, imperative, concrete, verifiable.
- desc should describe the style/voice/safety stance the answer actually exhibits.
- Do NOT include word count, paragraph structure, language choice, or first-person bans here.
  Those are handled elsewhere.
- Do NOT output explanations outside JSON."""


    user_prompt = (
        "GLOBAL OUTLINE (structure only; DO NOT invent rules from this):\n"
        f"{outline_str}\n\n"
        "TEXT SNIPPET (this is the FULL ANSWER content as given to the user;\n"
        "ALL requirements MUST be grounded in this text, do NOT hallucinate):\n"
        f"{answer_clean}\n\n"
        "Extract the global style/tone/safety constraints that the answer is ALREADY following.\n"
        "Return ONLY the JSON list.\n"
    )

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {_DEEPSEEK_API_KEY_DEFAULT}",
    }

    payload = {
        "model": _DEEPSEEK_MODEL,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        "temperature": 0.0,
        "max_tokens": 800,
    }

    try:
        resp = requests.post(
            _DEEPSEEK_ENDPOINT, headers=headers, data=json.dumps(payload), timeout=20
        )
        resp.raise_for_status()
        data = resp.json()
        content = data["choices"][0]["message"]["content"].strip()
        return content
    except Exception:
        # å…œåº•ï¼šè¿”å›ä¸€ä¸ªç©º JSON list å­—ç¬¦ä¸²ï¼Œè®©ä¸Šå±‚è§£ææ—¶å¾—åˆ° []
        return "[]"


# -------------------------------------------------
# ä¸»å…¥å£ï¼šç»“åˆç¡¬çº¦æŸ + è½¯çº¦æŸ
# -------------------------------------------------

def extract_global_constraints(response_text: str,
                               segmentation: Dict[str, Any]) -> List[ConstraintNode]:
    """
    Step3 ä¸»å…¥å£ï¼š

    1. åŸºäºå›ç­”æ–‡æœ¬ + segmentationï¼Œæ„é€ ç¡¬æ€§å…¨å±€çº¦æŸï¼ˆæœ¬åœ°å¯éªŒè¯ï¼‰ã€‚
       è¿™äº›çº¦æŸæ°¸è¿œä¼šå­˜åœ¨ï¼Œç¡®ä¿æˆ‘ä»¬è‡³å°‘èƒ½åšç¨‹åºåŒ–æ‰“åˆ†ã€‚

    2. è°ƒç”¨ deepseek æå–è½¯æ€§/è¯­æ°”/å®‰å…¨ç±»çš„å…¨å±€çº¦æŸï¼›å¯èƒ½è¿”å›0æ¡ï¼Œä¹Ÿå¯èƒ½å¤šæ¡ã€‚

    3. æŠŠäºŒè€…åˆå¹¶ï¼ŒæŒ‰é¡ºåºç¼–å· G1, G2, ... ï¼Œå¾—åˆ°æœ€ç»ˆçš„å…¨å±€çº¦æŸåˆ—è¡¨ã€‚

    æ³¨æ„ï¼š
    - ä¸å†å¼ºè¡Œç”Ÿæˆé€šç”¨fallbackçº¦æŸã€‚
    - è½¯æ€§çº¦æŸç¼ºå¸­æ—¶ï¼Œæˆ‘ä»¬ä»æœ‰ç¡¬æ€§çº¦æŸå¯ç”¨ã€‚
    """

    hard_nodes = _build_hard_global_constraints(response_text, segmentation)
    soft_raw_str = _call_deepseek_soft_constraints(response_text, segmentation)
    soft_items = extract_constraints(soft_raw_str)  # list[dict]

    soft_nodes: List[ConstraintNode] = []
    for item in soft_items:
        # extract_constraints() å·²ç»å°½é‡æ ‡å‡†åŒ–å­—æ®µåï¼šcid/desc/scope/verifier_spec
        desc = item.get("desc", "").strip()
        verifier_spec = item.get("verifier_spec", {}) or item.get("verifier", {}) or {}
        check_name = verifier_spec.get("check")
        args_obj = verifier_spec.get("args", {}) or {}

        if not desc or not check_name:
            continue

        soft_nodes.append(
            ConstraintNode(
                cid="TEMP",  # åç»­ç»Ÿä¸€é‡æ’ID
                desc=desc,
                scope="global",
                verifier_spec={"check": check_name, "args": args_obj},
                trace_to=None,
                derived_from="step3",
            )
        )

    # åˆå¹¶å¹¶é‡æ–°ç¼–å· cid
    all_nodes: List[ConstraintNode] = []
    for node in hard_nodes + soft_nodes:
        all_nodes.append(node)
    for idx, node in enumerate(all_nodes, start=1):
        node.cid = f"G{idx}"

    return all_nodes


if __name__ == "__main__":
    demo_resp = (
        "The modern space race is not only a technical contest but a geopolitical instrument. "
        "In this analysis, we outline historical context, assess key actors, and discuss future risks.\n\n"
        "First, we review how national prestige and commercial incentives shaped recent launches.\n\n"
        "Finally, we conclude with implications for global stability and practical next-step recommendations."
    )
    demo_seg = {
        "blocks": [
            {"block_id": "B1", "intent": "Opening / Context setup", "text_span": "..."},
            {"block_id": "B2", "intent": "Main Analysis", "text_span": "..."},
            {"block_id": "B3", "intent": "Conclusion / Outlook / Recommendation", "text_span": "..."},
        ],
        "order": ["B1", "B2", "B3"],
    }

    out_nodes = extract_global_constraints(demo_resp, demo_seg)
    print(json.dumps([
        {
            "cid": n.cid,
            "desc": n.desc,
            "scope": n.scope,
            "verifier_spec": n.verifier_spec,
            "derived_from": n.derived_from,
        } for n in out_nodes
    ], indent=2, ensure_ascii=False))