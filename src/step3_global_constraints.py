"""
step3_global_constraints.py

Step 3: å…¨å±€çº¦æŸæŠ½å– (Global Constraint Extraction)

- æˆ‘ä»¬æŠŠå…¨å±€çº¦æŸåˆ†æˆä¸¤ç±»ï¼š
  A. ç¡¬æ€§å¯ç¨‹åºæ ¡éªŒçš„å…¨å±€çº¦æŸï¼ˆhard global constraintsï¼‰
     ä¾‹å¦‚ï¼šæœ€å°‘è¯æ•°ã€å¿…é¡»åŒ…å«ç»“æ„æ®µè½ã€ç¦æ­¢ç¬¬ä¸€äººç§°ã€å¿…é¡»ä¸ºè‹±æ–‡ã€‚
     è¿™äº›å¯ä»¥ç›´æ¥ç”±æˆ‘ä»¬æœ¬åœ°ä»£ç ç»™å‡ºå¹¶é™„ä¸Š verifier_specï¼Œ
     ä¸ä¾èµ– LLMï¼Œå› æ­¤æ€»æ˜¯å¯ç”¨ï¼Œä¿è¯ä¸‹æ¸¸è¯„æµ‹æœ‰ç¨³å®šåŸºçº¿ã€‚

  B. è½¯æ€§ / è¯­æ°” / å®‰å…¨ / è´¨é‡ç±»å…¨å±€çº¦æŸï¼ˆsoft / semantic global constraintsï¼‰
     ä¾‹å¦‚ï¼šä¸­ç«‹åˆ†æè¯­æ°”ã€ä¸å¾—ç…½åŠ¨æ€§æ”»å‡»ã€è¾“å‡ºå¿…é¡»ä¿æŒä¸“ä¸šè€Œéæƒ…ç»ªåŒ–ã€‚
     è¿™äº›éœ€è¦è¯­ç”¨åˆ¤æ–­ï¼Œç»§ç»­è°ƒç”¨ deepseek ç”Ÿæˆæˆ–ç¡®è®¤ï¼Œ
     å¹¶ä¸ºå®ƒä»¬é™„ä¸Š LLM-based çš„ verifierï¼ˆå¦‚ tone_neutral_llm_judge, non_extremeness_judgeï¼‰ã€‚


è¾“å‡ºï¼šList[ConstraintNode]
- æ¯ä¸ª ConstraintNode:
    cid: å…¨å±€å”¯ä¸€IDï¼ˆG1, G2, ...ï¼‰
    desc: äººç±»å¯è¯»æè¿°
    scope: "global"
    verifier_spec: {"check": <fn-name>, "args": {...}}
    derived_from: "step3"

ä¾èµ–ï¼š
- deepseek-chat (ç”¨äºè½¯æ€§çº¦æŸ)
- ConstraintNode schema
- ç¡¬æ€§è§„åˆ™æ¥è‡ªæˆ‘ä»¬è‡ªå·±çš„å¯å‘å¼ï¼š
  - å­—æ•°ä¸‹é™ (min_word_count)
  - è¯­è¨€åˆ¤æ–­ (require_language)
  - ç»“æ„æ®µè½ (has_sections) [ä»…å½“å›ç­”æ˜æ˜¾åˆ†å—æ—¶]
  - ç¦æ­¢ç¬¬ä¸€äººç§° (forbid_first_person) [å¯é€‰]
"""

import json
import requests
from typing import List, Dict, Any

from .graph_schema import ConstraintNode
from .utils.parsing import extract_constraints
from .utils.text_clean import make_snippet, summarize_blocks_outline, clip

# Optional description templates (user-provided). We gracefully fall back if unavailable.
try:
    from .utils.templates import DESCS as _DESC_TEMPLATES  # type: ignore
except Exception:
    _DESC_TEMPLATES = {}

def _desc_from_tpl(key: str, default: str, **kwargs) -> str:
    """
    Pull a description template by key from user-provided templates. If the template value
    is a list or tuple, randomly select one candidate. Fall back to `default` if key not found.
    """
    tpl = _DESC_TEMPLATES.get(key, default)
    import random
    if isinstance(tpl, (list, tuple)) and tpl:
        tpl = random.choice(list(tpl))
    try:
        return str(tpl).format(**kwargs)
    except Exception:
        # final fallback: use default
        return default.format(**kwargs)

# Regex utilities for new hard constraints
import re
_PAR_SPLIT = re.compile(r"(?:\r?\n){2,}")
_MD_HEADING = re.compile(r"^(#{1,6})\s+\S", re.M)
_BULLET_MARK = re.compile(r"^(?:[-*]\s+|\d+\.\s+)", re.M)
_EMOJI = re.compile(r"[\U00010000-\U0010ffff]", re.UNICODE)

from .utils.deepseek_client import call_chat_completions, DeepSeekError
_DEEPSEEK_API_KEY_DEFAULT = ""
_DEEPSEEK_ENDPOINT = ""
_DEEPSEEK_MODEL = ""


# -------------------------------------------------
# å·¥å…·ï¼šä»å½“å‰å›ç­”ä¸­æ¨æµ‹ç¡¬æ€§å…¨å±€çº¦æŸåŸºçº¿
# -------------------------------------------------

def _estimate_word_count(text: str) -> int:
    """
    Estimate word count robustly for mixed Latin/CJK:
    - Latin tokens: \\w+ matches words/numbers/underscore
    - CJK: count individual Han characters (rough proxy)
    """
    tokens = re.findall(r"\w+", text)
    zh_chars = re.findall(r"[\u4e00-\u9fff]", text)
    return len(tokens) + len(zh_chars)


def _guess_language(text: str) -> str:
    """
    Heuristic primary language guess:
    - If Han characters dominate (â‰¥ 25 and â‰¥ 40% of visible letters), return 'zh'
    - Otherwise 'en'
    """
    zh_chars = re.findall(r"[\u4e00-\u9fff]", text)
    letters = re.findall(r"[A-Za-z\u4e00-\u9fff]", text)
    if len(zh_chars) >= 25 and len(zh_chars) >= 0.4 * max(1, len(letters)):
        return "zh"
    return "en"


def _has_intro_body_conclusion(segmentation: Dict[str, Any]) -> bool:
    """
    æ ¹æ® Step2 çš„ segmentation ç»“æœï¼Œçœ‹çœ‹æ˜¯å¦èƒ½è§‚å¯Ÿåˆ°å…¸å‹ç»“æ„ï¼š
    - å­˜åœ¨å¼€ç¯‡ç±»å— (Opening / Intro / Background / Context)
    - å­˜åœ¨ä¸»ä½“åˆ†æç±»å— (Main Analysis / Discussion / Evaluation / Argument)
    - å­˜åœ¨æ€»ç»“/å±•æœ›ç±»å— (Conclusion / Summary / Outlook / Recommendation)

    å¦‚æœè¿™äº›intentåŸºæœ¬å­˜åœ¨ï¼Œå°±å¯ä»¥ç”Ÿæˆä¸€ä¸ª has_sections çº¦æŸã€‚
    å¦åˆ™åˆ«å¼ºè¡Œè¦æ±‚ã€‚
    """
    intents = [blk.get("intent", "").lower() for blk in segmentation.get("blocks", [])]

    def any_contains(keys):
        return any(any(k in intent for k in keys) for intent in intents)

    has_opening = any_contains(["opening", "intro", "context", "background"])
    has_body = any_contains(["analysis", "discussion", "main", "argument", "evaluation"])
    has_conclusion = any_contains(["conclusion", "summary", "outlook", "recommendation"])

    return has_opening and has_body and has_conclusion

# ---- New hard constraint detectors ----
def _count_paragraphs(text: str) -> int:
    paras = [p for p in _PAR_SPLIT.split(text.strip()) if p.strip()]
    return len(paras)

def _detect_heading_levels(text: str):
    levels = set()
    for m in _MD_HEADING.finditer(text):
        levels.add(len(m.group(1)))
    return levels

def _detect_bullet_marker(text: str):
    marks = re.findall(r"^([-*]|\d+\.)\s+", text, re.M)
    if not marks:
        return None, False
    first = marks[0]
    mixed = any(m != first for m in marks[1:])
    return first, mixed

def _has_emojis(text: str) -> bool:
    return bool(_EMOJI.search(text))

def _detect_citation_style(text: str):
    # numeric [1], [12] vs author-year (Smith, 2021)
    has_numeric = bool(re.search(r"\[\d{1,3}\]", text))
    has_author_year = bool(re.search(r"\([A-Z][A-Za-z]+,?\s+\d{4}\)", text))
    if has_numeric and not has_author_year:
        return "numeric"
    if has_author_year and not has_numeric:
        return "author_year"
    return None

def _detect_decimal_places(text: str):
    nums = re.findall(r"\b\d+\.(\d+)\b", text)
    if len(nums) < 3:
        return None
    from collections import Counter
    cnt = Counter(len(s) for s in nums)
    most, freq = cnt.most_common(1)[0]
    if freq >= max(3, int(0.6 * len(nums))):
        return most
    return None

def _detect_date_format(text: str):
    if re.search(r"\b\d{4}-\d{2}-\d{2}\b", text):
        return "yyyy-mm-dd"
    if re.search(r"\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\s+\d{1,2},\s+\d{4}\b", text):
        return "mon dd, yyyy"
    return None

def _detect_contractions_en(text: str) -> int:
    # count common English contractions as a proxy
    return len(re.findall(r"\b(?:don't|doesn't|can't|won't|I'm|it's|that's|we're|they're|I've|you've|isn't|aren't|weren't|hasn't|haven't|shouldn't|couldn't|wouldn't)\b", text, re.I))

# --- Keyword and symbol format helpers ---
_EN_STOP = set("""
a an the and or but if then else when while for to of in on at by from as is are was were be been being this that these those with without into within across over under between among can could should would may might must do does did done doing have has had having not no nor so such very more most other same own just also than too rather quite
""".split())

def _extract_keywords_simple(text: str, lang: str) -> List[str]:
    """
    Lightweight keyword extractor to avoid heavy deps:
    - EN: count word frequencies, remove short tokens (<=3) & stopwords, pick top 1-3 distinct
    - ZH: return [] (we avoid low-quality heuristics for now)
    """
    if lang == "zh":
        return []
    words = re.findall(r"[A-Za-z][A-Za-z\-']+", text.lower())
    # strip apostrophes at the ends
    words = [w.strip("'").strip("-") for w in words]
    words = [w for w in words if len(w) > 3 and w not in _EN_STOP]
    if not words:
        return []
    from collections import Counter
    top = [w for (w, c) in Counter(words).most_common(8)]
    # keep order, take up to 3 unique
    out: List[str] = []
    for w in top:
        if w not in out:
            out.append(w)
        if len(out) == 3:
            break
    return out

_SYMBOL_CANDIDATES = [",", ":", "?", "!"]

def _choose_forbid_symbol(text: str) -> str | None:
    for s in _SYMBOL_CANDIDATES:
        if s not in text:
            return s
    return None


def _build_hard_global_constraints(response_text: str,
                                   segmentation: Dict[str, Any]) -> List[ConstraintNode]:
    """
    åŸºäºå¯è§‚æµ‹ä¿¡å·ï¼Œæ„é€ ç¨³å®šçš„ç¡¬æ€§å…¨å±€çº¦æŸèŠ‚ç‚¹ã€‚
    æˆ‘ä»¬ä¸ä¼šå¹»æƒ³ä¸å­˜åœ¨çš„è¦æ±‚ï¼Œåªæ ¹æ®æ–‡æœ¬æœ¬èº«çš„å®¢è§‚å±æ€§ï¼š
    - å­—æ•°ä¸‹é™ï¼šè®¾ä¸º floor(word_count * 0.8) å‘ä¸‹å–æ•´ï¼Œä½†è‡³å°‘ 100 è¯ã€‚
      ï¼ˆæ€è·¯ï¼šæˆ‘ä»¬å¸Œæœ›åç»­å›ç­”åˆ«æ¯”ç¤ºä¾‹çŸ­å¤ªå¤šï¼Œå¦åˆ™ä¸åˆæ ¼ï¼‰
    - è¯­è¨€ï¼šæ ¹æ®æ–‡æœ¬ä¸»è¯­è¨€ç”Ÿæˆ require_language(lang=...)
    - ç»“æ„æ®µè½ï¼šå¦‚æœ segmentation çœ‹èµ·æ¥æœ‰å¼€å¤´/ä¸»ä½“/ç»“è®ºï¼Œå°±è¦æ±‚ has_sections
    è¿™äº›éƒ½ä¼šè¢«æ ‡è®°ä¸º scope="global"ã€‚
    """
    nodes: List[ConstraintNode] = []
    cid_counter = 1
    added_categories: set[str] = set()
    def _add_node(category: str, node: ConstraintNode) -> bool:
        """
        Add a node only if this category hasn't been used yet.
        Returns True if added, False if skipped due to category cap.
        """
        if category in added_categories:
            return False
        nodes.append(node)
        added_categories.add(category)
        return True

    import random
    def _add_random_from(category: str, candidates: List[ConstraintNode]) -> int:
        """
        From a non-empty list of candidate nodes belonging to the same super-category,
        randomly select ONE and add it via _add_node. Returns 1 if added, else 0.
        """
        if not candidates:
            return 0
        choice = random.choice(candidates)
        if _add_node(category, choice):
            return 1
        return 0

    # -----------------------------
    # Phase A: collect candidates
    # -----------------------------

    length_candidates: List[ConstraintNode] = []
    language_candidates: List[ConstraintNode] = []
    structure_candidates: List[ConstraintNode] = []
    format_candidates: List[ConstraintNode] = []
    style_safety_candidates: List[ConstraintNode] = []

    # A1) Length candidates
    wc = _estimate_word_count(response_text)
    if wc > 0:
        min_words = max(100, int(wc * 0.85))
        max_words = int(wc * 1.20)
        has_minmax = "digit_format_min_max" in _DESC_TEMPLATES
        has_around = "digit_format_around" in _DESC_TEMPLATES
        has_min = "digit_format_min" in _DESC_TEMPLATES
        has_max = "digit_format_max" in _DESC_TEMPLATES

        if has_minmax:
            length_candidates.append(ConstraintNode(
                cid=f"G{cid_counter}",
                desc=_desc_from_tpl(
                    "digit_format_min_max",
                    "Keep the answer length between {min_words} and {max_words} words.",
                    min_words=min_words, max_words=max_words,
                ),
                scope="global",
                verifier_spec={"check": "word_count_between", "args": {"min_words": min_words, "max_words": max_words}},
                trace_to=None, derived_from="step3",
            ))
        if has_around:
            center = int(round(wc)); tol = 0.15
            length_candidates.append(ConstraintNode(
                cid=f"G{cid_counter}",
                desc=_desc_from_tpl(
                    "digit_format_around",
                    "Keep the answer length around {center} words (Â±{tol_pct}%).",
                    center=center, tol_pct=int(tol * 100),
                ),
                scope="global",
                verifier_spec={"check": "word_count_around", "args": {"center": center, "tolerance_pct": tol}},
                trace_to=None, derived_from="step3",
            ))
        if has_min:
            length_candidates.append(ConstraintNode(
                cid=f"G{cid_counter}",
                desc=_desc_from_tpl(
                    "digit_format_min",
                    "The answer must be at least {min_words} words long.",
                    min_words=min_words,
                ),
                scope="global",
                verifier_spec={"check": "min_word_count", "args": {"min_words": min_words}},
                trace_to=None, derived_from="step3",
            ))
        if has_max:
            length_candidates.append(ConstraintNode(
                cid=f"G{cid_counter}",
                desc=_desc_from_tpl(
                    "digit_format_max",
                    "Keep the answer under {max_words} words.",
                    max_words=max_words,
                ),
                scope="global",
                verifier_spec={"check": "max_word_count", "args": {"max_words": max_words}},
                trace_to=None, derived_from="step3",
            ))
        if not (has_minmax or has_around or has_min or has_max):
            length_candidates.append(ConstraintNode(
                cid=f"G{cid_counter}",
                desc=_desc_from_tpl(
                    "min_word_count",
                    "The answer must be at least {min_words} words long.",
                    min_words=min_words,
                ),
                scope="global",
                verifier_spec={"check": "min_word_count", "args": {"min_words": min_words}},
                trace_to=None, derived_from="step3",
            ))

    # A2) Language candidates
    lang = _guess_language(response_text)
    language_candidates.append(
        ConstraintNode(
            cid=f"G{cid_counter}",
            desc=_desc_from_tpl(
                "require_language_zh" if lang == "zh" else "require_language_en",
                ("The answer must be written primarily in Chinese." if lang == "zh"
                 else "The answer must be written primarily in English."),
            ),
            scope="global",
            verifier_spec={"check": "require_language", "args": {"lang": lang}},
            trace_to=None, derived_from="step3",
        )
    )
    if lang == "en":
        contractions = _detect_contractions_en(response_text)
        if contractions == 0:
            language_candidates.append(
                ConstraintNode(
                    cid=f"G{cid_counter}",
                    desc=_desc_from_tpl(
                        "avoid_contractions",
                        "Avoid contractions (use 'do not' instead of 'don't').",
                    ),
                    scope="global",
                    verifier_spec={"check": "avoid_contractions", "args": {}},
                    trace_to=None, derived_from="step3",
                )
            )

    # A3) Structure candidates
    if _has_intro_body_conclusion(segmentation):
        structure_candidates.append(
            ConstraintNode(
                cid=f"G{cid_counter}",
                desc=_desc_from_tpl(
                    "has_sections_intro_body_conclusion",
                    "The answer must include an Opening/Intro section, a Body/Main Analysis section, and a Conclusion/Outlook section in logical progression.",
                ),
                scope="global",
                verifier_spec={"check": "has_sections", "args": {"sections": ["Opening", "Body", "Conclusion"]}},
                trace_to=None, derived_from="step3",
            )
        )
    para_cnt = _count_paragraphs(response_text)
    if para_cnt >= 3:
        structure_candidates.append(
            ConstraintNode(
                cid=f"G{cid_counter}",
                desc=_desc_from_tpl(
                    "min_paragraphs",
                    "Organize the answer into at least {min_paras} paragraphs.",
                    min_paras=para_cnt,
                ),
                scope="global",
                verifier_spec={"check": "min_paragraphs", "args": {"min_paras": para_cnt}},
                trace_to=None, derived_from="step3",
            )
        )

    # A4) Format consistency candidates
    heading_levels = _detect_heading_levels(response_text)
    if heading_levels:
        levels_sorted = sorted(heading_levels)
        format_candidates.append(
            ConstraintNode(
                cid=f"G{cid_counter}",
                desc=_desc_from_tpl(
                    "heading_levels_only",
                    "Use consistent Markdown heading levels: only {levels}.",
                    levels=levels_sorted,
                ),
                scope="global",
                verifier_spec={"check": "heading_levels_only", "args": {"levels": levels_sorted}},
                trace_to=None, derived_from="step3",
            )
        )
    bullet, mixed = _detect_bullet_marker(response_text)
    if bullet:
        format_candidates.append(
            ConstraintNode(
                cid=f"G{cid_counter}",
                desc=_desc_from_tpl(
                    "bullet_style_consistent",
                    "Use a consistent list marker style ('{marker}'); do not mix list markers.",
                    marker=bullet,
                ),
                scope="global",
                verifier_spec={"check": "bullet_style_consistent", "args": {"marker": bullet}},
                trace_to=None, derived_from="step3",
            )
        )
    dec = _detect_decimal_places(response_text)
    if dec is not None:
        format_candidates.append(
            ConstraintNode(
                cid=f"G{cid_counter}",
                desc=_desc_from_tpl(
                    "decimal_places",
                    "Keep numeric values to {places} decimal places consistently.",
                    places=dec,
                ),
                scope="global",
                verifier_spec={"check": "decimal_places", "args": {"places": dec}},
                trace_to=None, derived_from="step3",
            )
        )
    dfmt = _detect_date_format(response_text)
    if dfmt:
        format_candidates.append(
            ConstraintNode(
                cid=f"G{cid_counter}",
                desc=_desc_from_tpl(
                    "date_format_iso" if dfmt=="yyyy-mm-dd" else "date_format_long",
                    ("Use the date format YYYY-MM-DD." if dfmt=="yyyy-mm-dd"
                     else "Use the date format 'Month DD, YYYY'."),
                ),
                scope="global",
                verifier_spec={"check": "date_format", "args": {"style": dfmt}},
                trace_to=None, derived_from="step3",
            )
        )

    # A5) Style & safety candidates
    lower_txt = response_text.lower()
    first_person_hits = any(token in lower_txt for token in [" i ", " we ", " my ", " our "])
    if not first_person_hits:
        style_safety_candidates.append(
            ConstraintNode(
                cid=f"G{cid_counter}",
                desc=_desc_from_tpl(
                    "forbid_first_person",
                    "Maintain an objective, third-person analytic voice; do not use first-person pronouns.",
                ),
                scope="global",
                verifier_spec={"check": "forbid_first_person", "args": {}},
                trace_to=None, derived_from="step3",
            )
        )
    if not _has_emojis(response_text):
        style_safety_candidates.append(
            ConstraintNode(
                cid=f"G{cid_counter}",
                desc=_desc_from_tpl(
                    "forbid_emojis",
                    "Do not use emojis or decorative unicode symbols.",
                ),
                scope="global",
                verifier_spec={"check": "forbid_emojis", "args": {}},
                trace_to=None, derived_from="step3",
            )
        )
    sym = _choose_forbid_symbol(response_text)
    if sym:
        style_safety_candidates.append(
            ConstraintNode(
                cid=f"G{cid_counter}",
                desc=_desc_from_tpl(
                    "symbol_format",
                    "Do not use the symbol '{symbol}'.",
                    symbol=sym,
                ),
                scope="global",
                verifier_spec={"check": "forbid_symbol", "args": {"symbol": sym}},
                trace_to=None, derived_from="step3",
            )
        )
    kws = _extract_keywords_simple(response_text, lang)
    if kws:
        style_safety_candidates.append(
            ConstraintNode(
                cid=f"G{cid_counter}",
                desc=_desc_from_tpl(
                    "keyword_format",
                    "Include the following keywords: {keywords}.",
                    keywords=", ".join(f"\"{k}\"" for k in kws),
                ),
                scope="global",
                verifier_spec={"check": "must_include_keywords", "args": {"keywords": kws}},
                trace_to=None, derived_from="step3",
            )
        )
    cite_style = _detect_citation_style(response_text)
    if cite_style:
        style_safety_candidates.append(
            ConstraintNode(
                cid=f"G{cid_counter}",
                desc=_desc_from_tpl(
                    "citation_style_numeric" if cite_style=="numeric" else "citation_style_author_year",
                    ("Use numeric bracket citations like [1], [2]." if cite_style=="numeric"
                     else "Use authorâ€“year citations like (Smith, 2021)."),
                ),
                scope="global",
                verifier_spec={"check": "citation_style", "args": {"style": cite_style}},
                trace_to=None, derived_from="step3",
            )
        )

    # -----------------------------
    # Phase B: randomly select one per super-category
    # -----------------------------
    cid_counter += _add_random_from("length", length_candidates)
    cid_counter += _add_random_from("language", language_candidates)
    cid_counter += _add_random_from("structure", structure_candidates)
    cid_counter += _add_random_from("format_consistency", format_candidates)
    cid_counter += _add_random_from("style_safety", style_safety_candidates)

    return nodes


# -------------------------------------------------
# LLM: ç”Ÿæˆè½¯æ€§/è¯­æ°”/å®‰å…¨ç±»å…¨å±€çº¦æŸ
# -------------------------------------------------


def _call_deepseek_soft_constraints(response_text: str,
                                    segmentation: Dict[str, Any]) -> str:
    """
    è°ƒç”¨ deepseek è®©å®ƒç»™å‡ºâ€œè¯­æ°” / å®‰å…¨ / é£æ ¼â€ç±»å…¨å±€çº¦æŸã€‚

    éå¸¸é‡è¦ï¼š
    - æˆ‘ä»¬ç°åœ¨è¦æ±‚å®ƒåªèƒ½æ ¹æ®å›ç­”æœ¬èº«å·²ç»å‘ˆç°å‡ºæ¥çš„é£æ ¼/è¯­æ°”/å§¿æ€æ¥æç‚¼çº¦æŸï¼Œ
      ä¸èƒ½è„‘è¡¥â€œç†æƒ³ä¸Šåº”è¯¥æ˜¯ä»€ä¹ˆæ ·â€ã€‚
    - æˆ‘ä»¬æä¾›ç»™å®ƒçš„ TEXT SNIPPET æ˜¯åŸæ–‡æœ¬èº«ï¼ˆä»…åšç©ºç™½è§„æ•´ï¼‰ï¼Œ
      ä¸æ‘˜è¦ã€ä¸æ”¹å†™ã€ä¸è‡ªåŠ¨æˆªæ–­ï¼›åªæœ‰åœ¨æç«¯è¿‡é•¿æ—¶æ‰ clip() ä»¥é˜² token çˆ†ç‚¸ã€‚
    - outline åªæ˜¯ç»“æ„ä½ç½®å‚è€ƒï¼Œä¸èƒ½ç”¨æ¥å‘æ˜æ²¡å‡ºç°çš„è¦æ±‚ã€‚

    æœŸæœ› deepseek è¾“å‡ºï¼šJSON listï¼Œæ¯é¡¹ç±»ä¼¼ï¼š
        {
          "desc": "The answer must maintain a neutral, analytical tone.",
          "verifier": {"check": "tone_neutral_llm_judge", "args": {}}
        }
    """

    # å¤„ç†åŸæ–‡ï¼šä¿æŒè¯­ä¹‰ï¼Œå»é™¤å¤šä½™ç©ºç™½ï¼Œä¸é»˜è®¤æˆªæ–­
    answer_clean = make_snippet(response_text)
    if len(answer_clean) > 20000:
        # æç«¯é•¿æ–‡æœ¬æ‰è§¦å‘æˆªæ–­ï¼›è¿™æ˜¯æ˜¾å¼çš„ã€å¯å®¡è®¡çš„å†…å®¹ä¸¢å¤±ç‚¹
        answer_clean = clip(answer_clean, 20000)

    outline_str = summarize_blocks_outline(segmentation)
    
    system_prompt = """You are an instruction analyst.
Your job is to infer ONLY global style/tone/safety requirements that the FULL ANSWER is ALREADY FOLLOWING.
You MUST base every requirement on observable evidence in the provided TEXT SNIPPET.
Do NOT invent idealized rules that are not clearly demonstrated in that text.
The OUTLINE is just structural context (which block does what), NOT evidence.
If you cannot justify a requirement from the snippet, you must NOT output it.

Soft global constraints are about tone, safety, stance, professional voice, neutrality, actionability, or analyst persona consistency across the entire answer.
Do NOT restate local factual obligations (e.g. "must list three risks") that only apply to one block; those belong to local block constraints, not global style.  ğŸ”

Every constraint must be grounded in observable evidence in the TEXT SNIPPET.
Do NOT invent requirements that do not clearly appear in the text.

You must return ONLY valid JSON: a list of objects.
Each object MUST have: {desc, verifier:{check,args}}.

About verifier.check:
- If one of these fits, use it:
  tone_neutral_llm_judge
  tone_negative_llm_judge
  non_extremeness_judge
  role_consistency_judge
  actionability_judge
- Otherwise, you MUST create a new descriptive snake_case name
  that reflects the requirement, e.g. "must_include_case_studies", "balanced_argumentation", "risk_mitigation_guidance".
  This is allowed.
Any new verifier.check you create MUST still describe a requirement that is clearly exhibited by the TEXT SNIPPET. ğŸ”
You are NOT allowed to invent a requirement that the snippet does not follow, just to create a new check name. ğŸ”

Rules for new verifier names:
- snake_case only [a-z0-9_]
- It must reflect the obligation in desc.
- args must be a JSON object (possibly empty) describing any parameters needed to check this rule, e.g. {"min_items": 3}.

If nothing applies, return an empty JSON list [].

Rules:
- desc must be English, imperative, concrete, verifiable.
- desc should describe the style/voice/safety stance the answer actually exhibits.
- Do NOT include word count, paragraph structure, language choice, or first-person bans here.
  Those are handled elsewhere.
- Do NOT output explanations outside JSON."""


    user_prompt = (
        "GLOBAL OUTLINE (structure only; DO NOT invent rules from this):\n"
        f"{outline_str}\n\n"
        "TEXT SNIPPET (this is the FULL ANSWER content as given to the user;\n"
        "ALL requirements MUST be grounded in this text, do NOT hallucinate):\n"
        f"{answer_clean}\n\n"
        "Extract the global style/tone/safety constraints that the answer is ALREADY following.\n"
        "Return ONLY the JSON list.\n"
    )

    try:
        content = call_chat_completions(
            messages=[
                {"role": "user", "content": user_prompt},
            ],
            system_prompt=system_prompt,
            temperature=0.0,
            max_tokens=800,
            timeout=20,
        ).strip()
        return content
    except DeepSeekError:
        return "[]"


# -------------------------------------------------
# ä¸»å…¥å£ï¼šç»“åˆç¡¬çº¦æŸ + è½¯çº¦æŸ
# -------------------------------------------------

def extract_global_constraints(response_text: str,
                               segmentation: Dict[str, Any]) -> List[ConstraintNode]:
    """
    Step3 ä¸»å…¥å£ï¼š

    1. åŸºäºå›ç­”æ–‡æœ¬ + segmentationï¼Œæ„é€ ç¡¬æ€§å…¨å±€çº¦æŸï¼ˆæœ¬åœ°å¯éªŒè¯ï¼‰ã€‚
       è¿™äº›çº¦æŸæ°¸è¿œä¼šå­˜åœ¨ï¼Œç¡®ä¿æˆ‘ä»¬è‡³å°‘èƒ½åšç¨‹åºåŒ–æ‰“åˆ†ã€‚

    2. è°ƒç”¨ deepseek æå–è½¯æ€§/è¯­æ°”/å®‰å…¨ç±»çš„å…¨å±€çº¦æŸï¼›å¯èƒ½è¿”å›0æ¡ï¼Œä¹Ÿå¯èƒ½å¤šæ¡ã€‚

    3. æŠŠäºŒè€…åˆå¹¶ï¼ŒæŒ‰é¡ºåºç¼–å· G1, G2, ... ï¼Œå¾—åˆ°æœ€ç»ˆçš„å…¨å±€çº¦æŸåˆ—è¡¨ã€‚

    æ³¨æ„ï¼š
    - ä¸å†å¼ºè¡Œç”Ÿæˆé€šç”¨fallbackçº¦æŸã€‚
    - è½¯æ€§çº¦æŸç¼ºå¸­æ—¶ï¼Œæˆ‘ä»¬ä»æœ‰ç¡¬æ€§çº¦æŸå¯ç”¨ã€‚
    """

    hard_nodes = _build_hard_global_constraints(response_text, segmentation)
    soft_raw_str = _call_deepseek_soft_constraints(response_text, segmentation)
    soft_items = extract_constraints(soft_raw_str)  # list[dict]

    soft_nodes: List[ConstraintNode] = []
    for item in soft_items:
        # extract_constraints() å·²ç»å°½é‡æ ‡å‡†åŒ–å­—æ®µåï¼šcid/desc/scope/verifier_spec
        desc = item.get("desc", "").strip()
        verifier_spec = item.get("verifier_spec", {}) or item.get("verifier", {}) or {}
        check_name = verifier_spec.get("check")
        args_obj = verifier_spec.get("args", {}) or {}

        if not desc or not check_name:
            continue

        soft_nodes.append(
            ConstraintNode(
                cid="TEMP",  # åç»­ç»Ÿä¸€é‡æ’ID
                desc=desc,
                scope="global",
                verifier_spec={"check": check_name, "args": args_obj},
                trace_to=None,
                derived_from="step3",
            )
        )

    # åˆå¹¶å¹¶é‡æ–°ç¼–å· cid
    all_nodes: List[ConstraintNode] = []
    for node in hard_nodes + soft_nodes:
        all_nodes.append(node)
    for idx, node in enumerate(all_nodes, start=1):
        node.cid = f"G{idx}"

    return all_nodes


if __name__ == "__main__":
    demo_resp = (
        "The modern space race is not only a technical contest but a geopolitical instrument. "
        "In this analysis, we outline historical context, assess key actors, and discuss future risks.\n\n"
        "First, we review how national prestige and commercial incentives shaped recent launches.\n\n"
        "Finally, we conclude with implications for global stability and practical next-step recommendations."
    )
    demo_seg = {
        "blocks": [
            {"block_id": "B1", "intent": "Opening / Context setup", "text_span": "..."},
            {"block_id": "B2", "intent": "Main Analysis", "text_span": "..."},
            {"block_id": "B3", "intent": "Conclusion / Outlook / Recommendation", "text_span": "..."},
        ],
        "order": ["B1", "B2", "B3"],
    }

    out_nodes = extract_global_constraints(demo_resp, demo_seg)
    print(json.dumps([
        {
            "cid": n.cid,
            "desc": n.desc,
            "scope": n.scope,
            "verifier_spec": n.verifier_spec,
            "derived_from": n.derived_from,
        } for n in out_nodes
    ], indent=2, ensure_ascii=False))